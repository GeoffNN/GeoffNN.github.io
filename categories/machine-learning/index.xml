<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on </title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 21 Apr 2020 21:58:14 +0200</lastBuildDate>
    
	<atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Frank-Wolfe</title>
      <link>/project/frank-wolfe/</link>
      <pubDate>Tue, 21 Apr 2020 21:58:14 +0200</pubDate>
      
      <guid>/project/frank-wolfe/</guid>
      <description>The Frank-Wolfe (FW) or Conditional Gradient algorithm is a family of first-order algorithms for constrained optimization. They seek to solve problems of the form:
$$\min_{x\in \mathcal{C}} f(x), $$
where $\mathcal{C}$ is a non-empty convex and compact set, and the objective function $f$ is differentiable, and often smooth. The gist is the following:
 Given a current iterate $x_t$, Minimize the linear approximation (i.e. the $1^\text{st}$ order Taylor expansion) of $f$ over $\mathcal C$ ; this minimum is attained on $s_t$, an extremal point of $\mathcal C$; Update the iterate as a convex combination: $x_{t+1} = x_t + \gamma_t (s_t - x_t)$ for a given step-size $\gamma_t$.</description>
    </item>
    
  </channel>
</rss>